{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement learning to detoxify fine tuned instruct google/flan-T5 peft model to align with human values.","metadata":{}},{"cell_type":"markdown","source":"This is continuation of previous notebook, due to lack of GPU resources task(text summarization) is splitted across two notebooks.\n\nObjective is to build reward and PPO model to detoxifying fine tuned peft model.\n\nUsed Meta AI roberta hate speech model to evaluate toxicity, based on toxicity value PPO model will update RL policy.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\nfrom datasets import load_dataset\nfrom peft import PeftModel, LoraConfig, TaskType\n\n# trl: Transformer Reinforcement Learning\nfrom trl import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\nfrom trl import create_reference_model\nfrom trl.core import LengthSampler\n\nimport torch\nimport evaluate\nimport numpy as np\nimport pandas as pd\n\n# tqdm library makes the loops show a smart progress meter.\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:47:12.515906Z","iopub.execute_input":"2023-08-28T17:47:12.516299Z","iopub.status.idle":"2023-08-28T17:47:25.416698Z","shell.execute_reply.started":"2023-08-28T17:47:12.516175Z","shell.execute_reply":"2023-08-28T17:47:25.415700Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name='google/flan-t5-base'\nhuggingface_dataset_name=\"knkarthick/dialogsum\"\ndataset_original=load_dataset(huggingface_dataset_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:47:25.418156Z","iopub.execute_input":"2023-08-28T17:47:25.418535Z","iopub.status.idle":"2023-08-28T17:47:28.637775Z","shell.execute_reply.started":"2023-08-28T17:47:25.418500Z","shell.execute_reply":"2023-08-28T17:47:28.636744Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e1661e01a949c1a4ae53bbdde630f3"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366e7bb98db94ba9b2b3996dd40381c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9647a7fb4e543d88f9c1f85a6839486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232a2eab0c0448f2a65bedaa5a58666e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04174d5a4e8046c5aea5dc3f3a71224d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e79638498d942e3af7cfc64e59b3160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2fbda8f623410397826ebacb8955b3"}},"metadata":{}}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    \" Function returns total model parameters and trainable parameters.\"\n    \n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:47:28.640704Z","iopub.execute_input":"2023-08-28T17:47:28.641409Z","iopub.status.idle":"2023-08-28T17:47:28.648258Z","shell.execute_reply.started":"2023-08-28T17:47:28.641348Z","shell.execute_reply":"2023-08-28T17:47:28.647241Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# data preprocessing: \n\ndevice=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ndef build_dataset(model_name, dataset_name, input_min_text_length, input_max_text_length):\n    \"Preprocess the dataset and split it into train and test parts\"\n    \n    dataset = load_dataset(dataset_name, split=\"train\")\n    \n    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n\n    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n    \n    def tokenize(sample):\n        \n        # Wrap each dialogue with the instruction.\n        prompt = f\"\"\"\n                 Summarize the following conversation.\n\n                 {sample[\"dialogue\"]}\n\n                 Summary:\n                 \"\"\"\n        \n        sample[\"input_ids\"] = tokenizer.encode(prompt)\n        \n        # This must be called \"query\", which is a requirement of our PPO library.\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    # Tokenize each dialogue.\n    dataset = dataset.map(tokenize, batched=False)\n    dataset.set_format(type=\"torch\")\n    \n    # Split the dataset into train and test parts.\n    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n\n    return dataset_splits\n\ndataset = build_dataset(model_name=model_name, dataset_name=huggingface_dataset_name, input_min_text_length=200, input_max_text_length=1000)\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:47:28.653251Z","iopub.execute_input":"2023-08-28T17:47:28.654451Z","iopub.status.idle":"2023-08-28T17:47:48.632864Z","shell.execute_reply.started":"2023-08-28T17:47:28.654411Z","shell.execute_reply":"2023-08-28T17:47:48.631778Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f062f0d3aed470089c198dfb2ec32ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf376dc40974735ac488f701b3aa741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a151161f7d2433a9b3219b6e995e60c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cbe344b8ea74b2d80eb3aad48034a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10022 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n        num_rows: 8017\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n        num_rows: 2005\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PPO model:","metadata":{}},{"cell_type":"code","source":"# Create PPO model :\n\nlora_config = LoraConfig(r=32, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\npeft_model = PeftModel.from_pretrained(model, '/kaggle/input/peft-model/peft_model', lora_config=lora_config, torch_dtype=torch.bfloat16, device_map=device, is_trainable=True)\npeft_model.to(device)\nprint(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n\nppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model, torch_dtype=torch.bfloat16, device_map=device, is_trainable=True)                                                             \nprint(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\nprint(ppo_model.v_head)\n\n# Creating reference PPO model to measure toxicity before and after detoxification.\nref_model = create_reference_model(ppo_model)\nref_model.to(device)\nprint(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:47:48.634405Z","iopub.execute_input":"2023-08-28T17:47:48.634764Z","iopub.status.idle":"2023-08-28T17:48:01.212781Z","shell.execute_reply.started":"2023-08-28T17:47:48.634728Z","shell.execute_reply":"2023-08-28T17:48:01.211454Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de1d3ea7f30c410aab7780a24b1b7904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f1e06e9aef498d9e2d719e8c9ab74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450a7d14002e45848a332e257a497b82"}},"metadata":{}},{"name":"stdout","text":"PEFT model parameters to be updated:\n\ntrainable model parameters: 3538944\nall model parameters: 251116800\npercentage of trainable model parameters: 1.41%\n\nPPO model parameters to be updated (ValueHead + 769 params):\n\ntrainable model parameters: 3539713\nall model parameters: 251117569\npercentage of trainable model parameters: 1.41%\n\nValueHead(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (summary): Linear(in_features=768, out_features=1, bias=True)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n)\nReference model parameters to be updated:\n\ntrainable model parameters: 0\nall model parameters: 251117569\npercentage of trainable model parameters: 0.00%\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reward Model:","metadata":{}},{"cell_type":"code","source":"toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\ntoxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=device)\ntoxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n\ntoxicity_evaluator = evaluate.load(\"toxicity\", toxicity_model_name, module_type=\"measurement\", toxic_label=\"hate\")\n\ndef evaluate_toxicity(model, toxicity_evaluator, tokenizer, dataset, num_samples):\n    \"Function evaluate toxicity of model completions\"\n    \n    max_new_tokens=100\n\n    toxicities = []\n    input_texts = []\n    for i, sample in tqdm(enumerate(dataset)):\n        input_text = sample[\"query\"]\n\n        if i > num_samples:\n            break\n            \n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device)\n        \n        generation_config = GenerationConfig(max_new_tokens=max_new_tokens, top_k=0.0, top_p=1.0, do_sample=True)\n\n        response_token_ids = model.generate(input_ids=input_ids, generation_config=generation_config)\n        \n        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n        \n        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n\n        toxicities.extend(toxicity_score[\"toxicity\"])\n\n    # Compute mean & std using np.\n    mean = np.mean(toxicities)\n    std = np.std(toxicities)\n        \n    return mean, std\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n\nmean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, toxicity_evaluator=toxicity_evaluator, tokenizer=tokenizer, dataset=dataset[\"test\"], num_samples=10)\n                                                                          \nprint(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:48:01.217265Z","iopub.execute_input":"2023-08-28T17:48:01.218193Z","iopub.status.idle":"2023-08-28T17:48:28.428258Z","shell.execute_reply.started":"2023-08-28T17:48:01.218089Z","shell.execute_reply":"2023-08-28T17:48:28.426823Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05429936d04e4e3d9f54d2fc6dea2b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c252d66134874a939d997adbd6210c21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3229bf99df65424fa823daacaaac3ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ef99d4dc8f41e6bbc0a359bebecb79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f5c54bbef040f7a67eea70c13c238a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f53ef2a449945659cbe1311148cb894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1cd29dda574cb6a4465acef760d585"}},"metadata":{}},{"name":"stderr","text":"11it [00:19,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"toxicity [mean, std] before detox: [0.04130103446500884, 0.045098608991112565]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Update RL policy using PPO:","metadata":{}},{"cell_type":"code","source":"# RL policy updation using PPO:\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\nlearning_rate=1.41e-5\nmax_ppo_epochs=1\nmini_batch_size=4\nbatch_size=16\n\nconfig = PPOConfig(model_name=model_name, learning_rate=learning_rate, ppo_epochs=max_ppo_epochs, mini_batch_size=mini_batch_size, batch_size=batch_size)\nppo_trainer = PPOTrainer(config=config, model=ppo_model, ref_model=ref_model, tokenizer=tokenizer, dataset=dataset[\"train\"], data_collator=collator)\n\noutput_min_length = 100\noutput_max_length = 400\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\nnot_hate_index=0\nsentiment_pipe = pipeline(\"sentiment-analysis\",  model=toxicity_model_name)\n\ngeneration_kwargs = {\"min_length\": 5, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True}\n  \nreward_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}\n\nmax_ppo_steps = 10\n\nfor step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    # Break when you reach max_steps.\n    if step >= max_ppo_steps:\n        break   \n    \n    prompt_tensors = batch[\"input_ids\"]\n\n    # Get response from FLAN-T5/PEFT LLM.\n    summary_tensors = []\n\n    for prompt_tensor in prompt_tensors:\n        max_new_tokens = output_length_sampler()        \n        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n        \n    # This needs to be called \"response\".\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n\n    # Compute reward outputs.\n    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n\n    # You use the `nothate` item because this is the score for the positive `nothate` class.\n    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n\n    # Run PPO step.\n    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n    ppo_trainer.log_stats(stats, batch, reward_tensors)\n    \n    print(f'objective/kl: {stats[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n    print('-'.join('' for x in range(100)))\n    \n    \nmean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, toxicity_evaluator=toxicity_evaluator, tokenizer=tokenizer, dataset=dataset[\"test\"], num_samples=100)\n                                              \nprint(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')","metadata":{"execution":{"iopub.status.busy":"2023-08-28T17:48:28.430989Z","iopub.execute_input":"2023-08-28T17:48:28.431388Z","iopub.status.idle":"2023-08-28T17:56:35.527758Z","shell.execute_reply.started":"2023-08-28T17:48:28.431341Z","shell.execute_reply":"2023-08-28T17:56:35.526754Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n1it [00:29, 29.72s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 29.292028427124023\nppo/returns/mean: -0.5290176868438721\nppo/policy/advantages_mean: 2.507438701115916e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"2it [01:03, 32.15s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 30.621055603027344\nppo/returns/mean: -0.5418417453765869\nppo/policy/advantages_mean: -9.766496944507708e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"3it [01:40, 34.47s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 32.923484802246094\nppo/returns/mean: -0.7347954511642456\nppo/policy/advantages_mean: 9.122857136389939e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"4it [02:14, 34.32s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 28.05332374572754\nppo/returns/mean: -0.37403279542922974\nppo/policy/advantages_mean: 9.219272012472857e-10\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"5it [02:48, 33.95s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 31.806900024414062\nppo/returns/mean: -0.6588640213012695\nppo/policy/advantages_mean: 1.4863599240300118e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"6it [03:19, 33.18s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 24.307518005371094\nppo/returns/mean: -0.18032237887382507\nppo/policy/advantages_mean: -5.815087078531178e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"7it [03:54, 33.59s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 28.29378890991211\nppo/returns/mean: -0.5968725085258484\nppo/policy/advantages_mean: -5.079876963520746e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"8it [04:27, 33.42s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 28.12511444091797\nppo/returns/mean: -0.36000287532806396\nppo/policy/advantages_mean: 8.678624041635885e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"9it [05:07, 35.66s/it]","output_type":"stream"},{"name":"stdout","text":"objective/kl: 27.942882537841797\nppo/returns/mean: -0.5314969420433044\nppo/policy/advantages_mean: -7.167169968624876e-09\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"10it [05:40, 34.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"objective/kl: 23.903675079345703\nppo/returns/mean: -0.33554959297180176\nppo/policy/advantages_mean: 1.0602102307188943e-08\n---------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"101it [02:24,  1.43s/it]","output_type":"stream"},{"name":"stdout","text":"toxicity [mean, std] after detox: [0.023523962623837387, 0.03528463366608066]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclusion:\n\nBefore detoxification mean and std was: [0.04130103446500884, 0.045098608991112565]\n\nAfter detoxification mean and std is: [0.023523962623837387, 0.03528463366608066]","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}