{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project name: Text Summarization.\n# LLM Model: google/flan-t5-base.\n# Dataset: knkarthick/dialogsum from HuggingFaceHub","metadata":{}},{"cell_type":"markdown","source":"# Project outline:\n\n### 1. Project objective\n### 2. Dataset details\n### 3. Prompt engineering\n### 4. Full fine tuning\n### 5. Fine tuning using PEFT technique\n### 6. Evaluation metric\n### 7. Conclusion\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Project objective:\n\nSummarization of conversational dialogue using Large language models and also includes understanding of fine tuning, Full-Fine tuning of LLMs and PEFT techniques to downstream LLMs for specific applications.","metadata":{}},{"cell_type":"markdown","source":"# 2. Dataset details:\n\nThese datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel. Most conversations take place between friends, colleagues, and between service providers and customers.\n\nDataset contains 4 fields.\n\n1. Dialogue: text of dialogue.\n2. summary: human written summary of the dialogue.\n3. topic: human written topic/one liner of the dialogue.\n4. id: unique file id of an example.\n\nThere are total 14460 conversations(dialogue-summary) and split is as follows.\n\nTrain samples: 12460\n\nValidation samples: 500\n\nTest samples: 1500","metadata":{}},{"cell_type":"markdown","source":"# 3. Prompt engineering:\n\nLooking for better dialogue summarization using different prompts.\n\n1. Zero-shot inference\n2. One-shot inference\n3. Few-shot inference","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport torchdata\nimport evaluate\nimport time\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:46:30.463806Z","iopub.execute_input":"2023-08-21T09:46:30.464169Z","iopub.status.idle":"2023-08-21T09:46:43.549011Z","shell.execute_reply.started":"2023-08-21T09:46:30.464135Z","shell.execute_reply":"2023-08-21T09:46:43.548025Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Huggingface dataset id and model name.\nHuggingface_dataset='knkarthick/dialogsum'\nmodel_name='google/flan-t5-base'\n\ndataset=load_dataset(Huggingface_dataset)\noriginal_model=AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer=AutoTokenizer.from_pretrained(model_name)\n\n# Since not having enough GPU memory, saving and running original, instruct and peft model from a local directory. \noutput_dir='/kaggle/working/original_model'\noriginal_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:47:34.356268Z","iopub.execute_input":"2023-08-21T09:47:34.356671Z","iopub.status.idle":"2023-08-21T09:47:48.226961Z","shell.execute_reply.started":"2023-08-21T09:47:34.356639Z","shell.execute_reply":"2023-08-21T09:47:48.218247Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80f4cedb9f141b8989ccfd32855e7cb"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e868e985a9fd45b08e4e1989c48a4245"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad941c4de3944f281b4d20922e18b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f47b75aa02344b9a1254b7c1cd22b7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90256f2d1f40446ea113c2844aad07ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cc3d7f2cb84910be710bf642bd7d8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0b9b01297742b5ab2f73979b988dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fc1aca34ed84177a11b6494705b5047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c5f4cba53d4f3fb3b59b54a1578e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b3264a97354baf941d336c38918449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832422713580492f995fb990d940e1bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0880f1331d248b0b318f5ee1fef982b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe37b33d404b4f408795b7b846112e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed52ffcf3136498d8ce3e30c44bad8fa"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/original_model/tokenizer_config.json',\n '/kaggle/working/original_model/special_tokens_map.json',\n '/kaggle/working/original_model/spiece.model',\n '/kaggle/working/original_model/added_tokens.json',\n '/kaggle/working/original_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    \"Function returns total parameters and trainable parameters\"\n    \n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    \n    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:49:15.046428Z","iopub.execute_input":"2023-08-21T09:49:15.046830Z","iopub.status.idle":"2023-08-21T09:49:15.056744Z","shell.execute_reply.started":"2023-08-21T09:49:15.046799Z","shell.execute_reply":"2023-08-21T09:49:15.055671Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Trainable model parameters: 247577856\nAll model parameters: 247577856\nPercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.1. Prompt engineering: Zero-shot inference.","metadata":{}},{"cell_type":"code","source":"def prompt_eng_zero_shot(index_number):\n    \" This function takes conversation/row index number and returns zero-shot inference output \"\n    \n    outputs=[]\n    \n    summary=dataset['test'][index_number]['summary']\n    dialogue=dataset['test'][index_number]['dialogue']\n\n    prompt1 = f\"\"\"Summarize the following conversation.\n                  {dialogue}\n                  summary:\n                  \"\"\"\n\n    prompt2 = f\"\"\"Extract the key takeaways from below conversation.\n                  {dialogue}\n                  summary:\n                  \"\"\"\n\n    prompt3 = f\"\"\"Conclusion of below dialogue.\n                  {dialogue}\n                  summary:\n                  \"\"\"\n\n    for prompt in [prompt1, prompt2, prompt3]:\n        inputs=tokenizer(prompt, return_tensors='pt').input_ids\n        output=tokenizer.decode(original_model.generate(inputs, max_new_tokens=200)[0], skip_special_tokens=True)\n        outputs.append(output)\n    \n    return print(f\"Prompt1 summary: {outputs[0]} \\nPrompt2 summary: {outputs[1]}  \\nPrompt3 summary: {outputs[2]}\")\n\nprompt_eng_zero_shot(index_number=300)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:49:18.482186Z","iopub.execute_input":"2023-08-21T09:49:18.482572Z","iopub.status.idle":"2023-08-21T09:49:27.839341Z","shell.execute_reply.started":"2023-08-21T09:49:18.482535Z","shell.execute_reply":"2023-08-21T09:49:27.838220Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Prompt1 summary: The President of the United States is a man of faith in Trump. \nPrompt2 summary: The president of the United States is a man of faith in Trump.  \nPrompt3 summary: Person1: I cannot imagine if Trump were to be our President again. Person2: I have nothing but faith in Trump.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.2. Prompt engineering: One-shot inference","metadata":{}},{"cell_type":"code","source":"def prompt_eng_one_shot(index_number_full, index_number_summarize):\n    \" This function takes conversation/row index number and returns one-shot inference output \"\n    \n    outputs=[]\n    \n    summary1=dataset['test'][index_number_full]['summary']\n    dialogue1=dataset['test'][index_number_full]['dialogue']\n    \n    summary2=dataset['test'][index_number_summarize]['summary']\n    dialogue2=dataset['test'][index_number_summarize]['dialogue']\n    \n    prompt1 = f\"\"\"Summarize the following conversation.\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Summarize the following conversation.\n                  {dialogue2}\n                  summary:\n                  \"\"\"\n\n    prompt2 = f\"\"\"Extract the key takeaways from below conversation.\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Extract the key takeaways from below conversation.\n                  {dialogue2}\n                  summary:\n                  \"\"\"\n\n    prompt3 = f\"\"\"Conclusion of below dialogue.\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Conclusion of below dialogue\n                  {dialogue2}\n                  summary:\n                  \"\"\"\n\n    for prompt in [prompt1, prompt2, prompt3]:\n        inputs=tokenizer(prompt, return_tensors='pt').input_ids\n        output=tokenizer.decode(original_model.generate(inputs, max_new_tokens=200)[0], skip_special_tokens=True)\n        outputs.append(output)\n    \n    return print(f\"Prompt1 summary: {outputs[0]} \\nPrompt2 summary: {outputs[1]}  \\nPrompt3 summary: {outputs[2]}\")\n\nprompt_eng_one_shot(index_number_full=200, index_number_summarize=300)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:49:44.607249Z","iopub.execute_input":"2023-08-21T09:49:44.607658Z","iopub.status.idle":"2023-08-21T09:50:05.099910Z","shell.execute_reply.started":"2023-08-21T09:49:44.607622Z","shell.execute_reply":"2023-08-21T09:50:05.098810Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Prompt1 summary: #Person1: I am not sure if Trump is the right person to be our President again. #Person2: I am not sure if he is the right person to be our President again. #Person1: I am not sure if he is the right person to be our President again. #Person2: I am not sure if he is the right person to be our President again. \nPrompt2 summary: Person1: I cannot imagine if Trump were to be our President again.  \nPrompt3 summary: Person1: I cannot imagine if Trump were to be our President again.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.3. Prompt engineering: Few-shot inference\n\nIn few-shot inference, we have to supply more than one dialogue-summary pair hence considering 3 pairs.\n\nIf model is not giving better results for 5 or 6 dialogue-summary pairs then its better to go for fine tuning technique.","metadata":{}},{"cell_type":"code","source":"def prompt_eng_few_shot(index_number_full, index_number_summarize):\n    \n    \" This function takes conversation/row index number and returns few-shot inference output \"\n    \n    outputs=[]\n    \n    summary1=dataset['test'][index_number_full[0]]['summary']\n    dialogue1=dataset['test'][index_number_full[0]]['dialogue']\n    \n    summary2=dataset['test'][index_number_full[1]]['summary']\n    dialogue2=dataset['test'][index_number_full[1]]['dialogue']\n    \n    summary3=dataset['test'][index_number_full[2]]['summary']\n    dialogue3=dataset['test'][index_number_full[2]]['dialogue']\n    \n    summary4=dataset['test'][index_number_summarize]['summary']\n    dialogue4=dataset['test'][index_number_summarize]['dialogue']\n    \n    prompt1 = f\"\"\"Summarize the following conversation.\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Summarize the following conversation.\n                  {dialogue2}\n                  summary:{summary2}\n                  \n                  Summarize the following conversation.\n                  {dialogue3}\n                  summary:{summary3}\n                  \n                  Summarize the following conversation.\n                  {dialogue4}\n                  summary:\n                  \"\"\"\n\n    prompt2 = f\"\"\"Extract the key takeaways from below conversation.\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Extract the key takeaways from below conversation.\n                  {dialogue2}\n                  summary:{summary2}\n                  \n                  Extract the key takeaways from below conversation.\n                  {dialogue3}\n                  summary:{summary3}\n                  \n                  Extract the key takeaways from below conversation.\n                  {dialogue4}\n                  summary:\n                  \"\"\"\n\n    prompt3 = f\"\"\"Conclusion of below dialogue\n                  {dialogue1}\n                  summary:{summary1}\n                  \n                  Conclusion of below dialogue\n                  {dialogue2}\n                  summary:{summary2}\n                  \n                  Conclusion of below dialogue\n                  {dialogue3}\n                  summary:{summary3}\n                  \n                  Conclusion of below dialogue\n                  {dialogue4}\n                  summary:\n                  \"\"\"\n\n    for prompt in [prompt1, prompt2, prompt3]:\n        inputs=tokenizer(prompt, return_tensors='pt').input_ids\n        output=tokenizer.decode(original_model.generate(inputs, max_new_tokens=200)[0], skip_special_tokens=True)\n        outputs.append(output)\n    \n    return print(f\"Prompt1 summary: {outputs[0]} \\nPrompt2 summary: {outputs[1]}  \\nPrompt3 summary: {outputs[2]}\")\n\nprompt_eng_few_shot(index_number_full=[100, 200, 400], index_number_summarize=300)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:50:13.373292Z","iopub.execute_input":"2023-08-21T09:50:13.373849Z","iopub.status.idle":"2023-08-21T09:50:53.097919Z","shell.execute_reply.started":"2023-08-21T09:50:13.373813Z","shell.execute_reply":"2023-08-21T09:50:53.096741Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Prompt1 summary: The President of the United States is a man of faith in Trump. \nPrompt2 summary: The President of the United States is a man of faith in Trump.  \nPrompt3 summary: Person1 is proud to say that he is our President, and he will be really happy if he could be re-elected.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Takeway from prompt engineering technique:\nZero-shot: completions are single line and informative.\n\nOne-shot: more or less all completions are very similar for different prompts.\n\nFew-shot: Completions are more or less similar to zero shot inferences.","metadata":{}},{"cell_type":"markdown","source":"# 4. Full fine tuning:\nIn Full fine tuning, understanding the behaviour of model after full fine tuning, which means there is a chances that most of the parameters will get updated hence could expect coherent and reliable completions.\n\nThere is enough resources(GPU) requirement involved in full fine tuning activity, hence considered small dataset to train model. There is already exist a fully trained original model hence that model will be used for inferences.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    \n    \"This function helps to tokenize prompt-summary pairs and save their tensor id in dataset.\"\n    \n    start_prompt='Summarize the following conversation. \\n\\n'\n    end_prompt='\\n\\nSummary:'\n    prompt=[start_prompt+dialogue+end_prompt for dialogue in example['dialogue']]\n    example['input_ids']=tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n    example['labels']=tokenizer(prompt, padding=\"max_length\",truncation=True, return_tensors='pt').input_ids\n    \n    return example\n\ntokenized_dataset=dataset.map(tokenize_function, batched=True)\ntokenized_dataset=tokenized_dataset.remove_columns(['id', 'topic', 'dialogue', 'summary'])\ntokenized_datasets = tokenized_dataset.filter(lambda example, index: index % 100 == 0, with_indices=True)\n\n# Training dataset shape.\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:52:15.020811Z","iopub.execute_input":"2023-08-21T09:52:15.021379Z","iopub.status.idle":"2023-08-21T09:52:43.136054Z","shell.execute_reply.started":"2023-08-21T09:52:15.021336Z","shell.execute_reply":"2023-08-21T09:52:43.135083Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (125, 2)\nValidation: (5, 2)\nTest: (15, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir=f'./dialogue-summary-training-{str(int(time.time()))}'\n\n# Defining training configurations.\ntraining_args=TrainingArguments(output_dir=output_dir, learning_rate=1e-5, num_train_epochs=1, weight_decay=0.01, logging_steps=1, max_steps=1)\ntrainer=Trainer(model=original_model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['validation'])\n\n# original model training.\ntrainer.train()\n\n# Saving trained model and tokenizer in a local directory.\nmodel_path='/kaggle/working/full_fine_model'\ntrainer.model.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:52:53.028814Z","iopub.execute_input":"2023-08-21T09:52:53.029201Z","iopub.status.idle":"2023-08-21T09:54:06.021476Z","shell.execute_reply.started":"2023-08-21T09:52:53.029169Z","shell.execute_reply":"2023-08-21T09:54:06.020564Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230821_095329-uzuxzaeb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahadevgadge45/huggingface/runs/uzuxzaeb' target=\"_blank\">toasty-breeze-25</a></strong> to <a href='https://wandb.ai/mahadevgadge45/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahadevgadge45/huggingface' target=\"_blank\">https://wandb.ai/mahadevgadge45/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahadevgadge45/huggingface/runs/uzuxzaeb' target=\"_blank\">https://wandb.ai/mahadevgadge45/huggingface/runs/uzuxzaeb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>27.625000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/full_fine_model/tokenizer_config.json',\n '/kaggle/working/full_fine_model/special_tokens_map.json',\n '/kaggle/working/full_fine_model/spiece.model',\n '/kaggle/working/full_fine_model/added_tokens.json',\n '/kaggle/working/full_fine_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"def original_instruct_model_prediction(index): \n    \" This model takes datset index as input and returns original and instruct model completions.\"\n    \n    # Below instruct model trained on 1500 datapoints and which is different than a model trained above. Above instruct model is trained on 125 datapoints, which really doesn't affect almost parameters.\n    instruct_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/input/instruct-model', torch_dtype=torch.bfloat16) \n    original_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/original_model', torch_dtype=torch.bfloat16)\n    \n    dialogue = dataset['test'][index]['dialogue']\n    human_baseline_summary = dataset['test'][index]['summary']\n\n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    {dialogue}\n\n    Summary:\n    \"\"\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    \n    return print(f\"Human_baseline_summary:{human_baseline_summary} \\n\\nOriginal_model_output:{original_model_text_output} \\n\\nInstruct_model_output:{instruct_model_text_output}\")\n\noriginal_instruct_model_prediction(index=200)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T09:58:03.637521Z","iopub.execute_input":"2023-08-21T09:58:03.637931Z","iopub.status.idle":"2023-08-21T09:58:21.004072Z","shell.execute_reply.started":"2023-08-21T09:58:03.637899Z","shell.execute_reply":"2023-08-21T09:58:21.002954Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Human_baseline_summary:#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system. \n\nOriginal_model_output:#Person1#: I'm thinking of upgrading my computer. \n\nInstruct_model_output:#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Fine tuning using PEFT technique:\n\nAdvantage of using PEFT method is:\n\n1. Requires less GPU memory.\n2. Only few % of parameters will get updated which is most required factor.\n3. Overcoming catastrophic forgetting.\n4. PEFT model results are very low deviated from instruct model and very much evolved from original/base model.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType, PeftConfig\n\nlora_config=LoraConfig(r=32, lora_alpha=32, target_modules=['q','v'], lora_dropout=0.05, bias='none', task_type=TaskType.SEQ_2_SEQ_LM)\n\npeft_model=get_peft_model(original_model, lora_config)\n\nprint(print_number_of_trainable_model_parameters(peft_model))\n\n# Train PEFT model.\noutput_dir=f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_traning_args=TrainingArguments(output_dir=output_dir, auto_find_batch_size=True, learning_rate=1e-3, num_train_epochs=1, logging_steps=1, max_steps=1)\npeft_trainer=Trainer(model=peft_model, args=peft_traning_args, train_dataset=tokenized_datasets['train'])\npeft_trainer.train()\n\n# Saving peft_model in a local directory.\npeft_model_path='/kaggle/working/peft_model'\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T10:04:07.616658Z","iopub.execute_input":"2023-08-21T10:04:07.617049Z","iopub.status.idle":"2023-08-21T10:04:10.582359Z","shell.execute_reply.started":"2023-08-21T10:04:07.617018Z","shell.execute_reply":"2023-08-21T10:04:10.581221Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Trainable model parameters: 3538944\nAll model parameters: 251116800\nPercentage of trainable model parameters: 1.41%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>27.625000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/peft_model/tokenizer_config.json',\n '/kaggle/working/peft_model/special_tokens_map.json',\n '/kaggle/working/peft_model/spiece.model',\n '/kaggle/working/peft_model/added_tokens.json',\n '/kaggle/working/peft_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"/kaggle/input/peft-model/peft_model\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\npeft_model = PeftModel.from_pretrained(model, peft_model_id, is_trainable=False)\npeft_model.eval()\n\nprint(\"Peft model loaded\")","metadata":{"execution":{"iopub.status.busy":"2023-08-21T10:06:10.861583Z","iopub.execute_input":"2023-08-21T10:06:10.862021Z","iopub.status.idle":"2023-08-21T10:06:17.455037Z","shell.execute_reply.started":"2023-08-21T10:06:10.861987Z","shell.execute_reply":"2023-08-21T10:06:17.454061Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Peft model loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"def original_instruct_peft_model_prediction(index): \n    \" This model takes datset index as input and returns original, instruct and peft model completions.\"\n    \n    instruct_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/input/instruct-model', torch_dtype=torch.bfloat16)\n    original_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/original_model', torch_dtype=torch.bfloat16)\n    \n    dialogue = dataset['test'][index]['dialogue']\n    human_baseline_summary = dataset['test'][index]['summary']\n\n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    {dialogue}\n\n    Summary:\n    \"\"\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    \n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n    \n    return print(f\"Human_baseline_summary:{human_baseline_summary} \\n\\nOriginal_model_output:{original_model_text_output} \\n\\nInstruct_model_output:{instruct_model_text_output} \\n\\nPeft_model_output:{peft_model_text_output}\")\n\noriginal_instruct_peft_model_prediction(index=200)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T10:06:40.802072Z","iopub.execute_input":"2023-08-21T10:06:40.802452Z","iopub.status.idle":"2023-08-21T10:07:03.446345Z","shell.execute_reply.started":"2023-08-21T10:06:40.802421Z","shell.execute_reply":"2023-08-21T10:07:03.445262Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Human_baseline_summary:#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system. \n\nOriginal_model_output:#Person1#: I'm thinking of upgrading my computer. \n\nInstruct_model_output:#Person1# suggests #Person2# upgrading #Person2#'s system, hardware, and CD-ROM drive. #Person2# thinks it's great. \n\nPeft_model_output:#Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Evaluation metric:","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\ndef generate_model_summaries(index):\n    \n    \"This function takes input as index range and returns instruct and peft model completions.\"\n    \n    instruct_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/input/instruct-model', torch_dtype=torch.bfloat16)\n    original_model=AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/original_model', torch_dtype=torch.bfloat16)\n    peft_model = PeftModel.from_pretrained(model, peft_model_id, is_trainable=False)\n    \n    dialogues = dataset['test'][index[0]:index[1]]['dialogue']\n    human_baseline_summaries = dataset['test'][index[0]:index[1]]['summary']\n\n    original_model_summaries = []\n    instruct_model_summaries = []\n    peft_model_summaries = []\n\n    for idx, dialogue in enumerate(dialogues):\n        \n        prompt = f\"\"\"\n                 Summarize the following conversation.\n\n                {dialogue}\n\n                Summary: \"\"\"\n\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n        human_baseline_text_output = human_baseline_summaries[idx]\n        original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n        original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n        instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n        instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n        peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n        peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n        original_model_summaries.append(original_model_text_output)\n        instruct_model_summaries.append(instruct_model_text_output)\n        peft_model_summaries.append(peft_model_text_output)\n        zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n    \n    original_model_results = rouge.compute(predictions=original_model_summaries, references=human_baseline_summaries[0:len(original_model_summaries)], use_aggregator=True, use_stemmer=True)\n    instruct_model_results = rouge.compute(predictions=instruct_model_summaries, references=human_baseline_summaries[0:len(instruct_model_summaries)], use_aggregator=True, use_stemmer=True)\n    peft_model_results = rouge.compute(predictions=peft_model_summaries, references=human_baseline_summaries[0:len(peft_model_summaries)], use_aggregator=True, use_stemmer=True)\n    \n    return zipped_summaries, original_model_results, instruct_model_results, peft_model_results\n    \nzipped_summaries, original_model_results, instruct_model_results, peft_model_results = generate_model_summaries([0,100])    \n\ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n\nprint(f'ORIGINAL MODEL:{original_model_results} \\n\\nINSTRUCT MODEL:{instruct_model_results} \\n\\nPEFT MODEL:{peft_model_results}')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T10:07:18.293828Z","iopub.execute_input":"2023-08-21T10:07:18.294355Z","iopub.status.idle":"2023-08-21T10:30:04.214356Z","shell.execute_reply.started":"2023-08-21T10:07:18.294294Z","shell.execute_reply":"2023-08-21T10:30:04.213254Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a702ce5cd294172b285190560a46ba9"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:{'rouge1': 0.22008050937450813, 'rouge2': 0.060234779819727094, 'rougeL': 0.1905827047052721, 'rougeLsum': 0.19015343359756315} \n\nINSTRUCT MODEL:{'rouge1': 0.3840066445307293, 'rouge2': 0.14574678121958184, 'rougeL': 0.3057682999785146, 'rougeLsum': 0.3057114144860572} \n\nPEFT MODEL:{'rouge1': 0.3966762315764254, 'rouge2': 0.14731879414729326, 'rougeL': 0.3149559315358753, 'rougeLsum': 0.31460735226341496}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')\n    \nprint(\"Percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T10:30:52.029588Z","iopub.execute_input":"2023-08-21T10:30:52.029972Z","iopub.status.idle":"2023-08-21T10:30:52.045125Z","shell.execute_reply.started":"2023-08-21T10:30:52.029940Z","shell.execute_reply":"2023-08-21T10:30:52.043812Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 17.66%\nrouge2: 8.71%\nrougeL: 12.44%\nrougeLsum: 12.45%\nPercentage improvement of PEFT MODEL over INSTRUCT MODEL\nrouge1: 1.27%\nrouge2: 0.16%\nrougeL: 0.92%\nrougeLsum: 0.89%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 7. Conclusion:\n\n1. From above improvement values, there is no big difference in performance between instruct and peft model considering several other advantages(GPU memory and etc) of peft_model. Hence PEFT model will be our final model.\n\n2. Most cases PEFT model gives good performance in comparison with Instruct model(little compromise in performance) with good amount of samples/datapoints(~1500).","metadata":{"execution":{"iopub.status.busy":"2023-08-20T17:10:59.291700Z","iopub.status.idle":"2023-08-20T17:10:59.292413Z","shell.execute_reply.started":"2023-08-20T17:10:59.292158Z","shell.execute_reply":"2023-08-20T17:10:59.292182Z"}}},{"cell_type":"code","source":"### Note: There are some computational challenges like GPU and memory constriants may lead model to less accurate. ","metadata":{},"execution_count":null,"outputs":[]}]}